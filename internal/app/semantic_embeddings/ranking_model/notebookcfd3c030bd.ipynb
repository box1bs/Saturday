{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-16T11:16:31.027075Z",
     "iopub.status.busy": "2025-09-16T11:16:31.026704Z",
     "iopub.status.idle": "2025-09-16T11:16:44.599927Z",
     "shell.execute_reply": "2025-09-16T11:16:44.599220Z",
     "shell.execute_reply.started": "2025-09-16T11:16:31.027043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e838c8ac45a74b08b4ca3671a25b1e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f64461e30fc480ab2bde9fe199ca53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1.1/validation-00000-of-00001.parquet:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372c0d6f99e74f53b75f45764c3e42ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1.1/train-00000-of-00001.parquet:   0%|          | 0.00/175M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398d1b0267aa4134a0e3461b89a86ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1.1/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a3981714b446819a3fa6a8550e1cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10047 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d723d278104181bc4bf527e51a2607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/82326 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e553443d984c619a764cf127c44898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/9650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"microsoft/ms_marco\", \"v1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация библиотек и глобальных переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:55:01.726142Z",
     "iopub.status.busy": "2025-09-16T11:55:01.725431Z",
     "iopub.status.idle": "2025-09-16T11:55:02.700643Z",
     "shell.execute_reply": "2025-09-16T11:55:02.699898Z",
     "shell.execute_reply.started": "2025-09-16T11:55:01.726115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = ds['train']\n",
    "valid = ds['validation']\n",
    "\n",
    "import hashlib\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedder.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка *тренировочных данных*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:18:20.683210Z",
     "iopub.status.busy": "2025-09-16T11:18:20.682592Z",
     "iopub.status.idle": "2025-09-16T11:38:57.103824Z",
     "shell.execute_reply": "2025-09-16T11:38:57.103010Z",
     "shell.execute_reply.started": "2025-09-16T11:18:20.683182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec11ed528e24f90862aa4794edf428e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19591 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd3046c475a439191e2e1eff5cfbc6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_text_hash(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "passage_hash_to_text = {}\n",
    "for ex in train:\n",
    "    for ptext in ex['passages']['passage_text']:\n",
    "        p_hash = get_text_hash(ptext)\n",
    "        if p_hash not in passage_hash_to_text:\n",
    "            passage_hash_to_text[p_hash] = ptext\n",
    "\n",
    "queries = {ex['query_id']: ex['query'] for ex in train}\n",
    "\n",
    "passage_ids = list(passage_hash_to_text.keys())\n",
    "passage_texts = list(passage_hash_to_text.values())\n",
    "passage_embeddings = embedder.encode(passage_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "passage_emb_map = {pid: emb for pid, emb in zip(passage_ids, passage_embeddings)}\n",
    "\n",
    "query_ids = list(queries.keys())\n",
    "query_texts = list(queries.values())\n",
    "query_embeddings = embedder.encode(query_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "query_emb_map = {qid: emb for qid, emb in zip(query_ids, query_embeddings)}\n",
    "\n",
    "stemmed_passages = {}\n",
    "for idx, passage in passage_hash_to_text.items():\n",
    "    processed_text = \" \".join([stemmer.stem(token) for token in passage.lower().split() if token.isalpha()])\n",
    "    stemmed_passages[idx] = processed_text\n",
    "\n",
    "stemmed_query = {}\n",
    "for idx, query in queries.items():\n",
    "    processed_query = \" \".join([stemmer.stem(token) for token in query.lower().split() if token.isalpha()])\n",
    "    stemmed_query[idx] = processed_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка *валидационных данных* для оценки качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:43:26.280239Z",
     "iopub.status.busy": "2025-09-16T11:43:26.279916Z",
     "iopub.status.idle": "2025-09-16T11:46:04.934545Z",
     "shell.execute_reply": "2025-09-16T11:46:04.933793Z",
     "shell.execute_reply.started": "2025-09-16T11:43:26.280212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d1bcbf21e34ca8aa6ae68ad413f74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5547fadd5e4a6c868a7a26b6d05b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "queries_valid = {ex['query_id']: ex['query'] for ex in valid}\n",
    "\n",
    "stemmed_query_valid = {\n",
    "    idx: \" \".join([stemmer.stem(token) for token in query.lower().split() if token.isalpha()])\n",
    "    for idx, query in queries_valid.items()\n",
    "}\n",
    "\n",
    "query_embeddings_valid = embedder.encode(list(queries_valid.values()), convert_to_tensor=True, show_progress_bar=True)\n",
    "query_emb_map_valid = {qid: emb for qid, emb in zip(queries_valid.keys(), query_embeddings_valid)}\n",
    "\n",
    "passage_hash_to_text_valid = {}\n",
    "for ex in valid:\n",
    "    if 'passages' in ex and ex['passages']['passage_text']:\n",
    "        for ptext in ex['passages']['passage_text']:\n",
    "            p_hash = get_text_hash(ptext)\n",
    "            if p_hash not in passage_hash_to_text_valid:\n",
    "                passage_hash_to_text_valid[p_hash] = ptext\n",
    "\n",
    "stemmed_passages_valid = {\n",
    "    idx: \" \".join([stemmer.stem(token) for token in passage.lower().split() if token.isalpha()])\n",
    "    for idx, passage in passage_hash_to_text_valid.items()\n",
    "}\n",
    "passage_embeddings_valid = embedder.encode(list(passage_hash_to_text_valid.values()), convert_to_tensor=True, show_progress_bar=True)\n",
    "passage_emb_map_valid = {p_hash: emb for p_hash, emb in zip(passage_hash_to_text_valid.keys(), passage_embeddings_valid)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:51:03.173520Z",
     "iopub.status.busy": "2025-09-16T11:51:03.172892Z",
     "iopub.status.idle": "2025-09-16T11:51:03.184758Z",
     "shell.execute_reply": "2025-09-16T11:51:03.183601Z",
     "shell.execute_reply.started": "2025-09-16T11:51:03.173493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_query_density(query_tokens, passage_tokens):\n",
    "    if not passage_tokens:\n",
    "        return 0\n",
    "    \n",
    "    query_token_set = set(query_tokens)\n",
    "    term_frequency = sum(1 for token in passage_tokens if token in query_token_set)\n",
    "\n",
    "    return term_frequency / len(passage_tokens)\n",
    "\n",
    "def calculate_term_proximity(query_tokens, passage_tokens):\n",
    "    if len(set(query_tokens)) < 2:\n",
    "        return 0\n",
    "\n",
    "    positions = {token: [] for token in set(query_tokens)}\n",
    "    for i, p_token in enumerate(passage_tokens):\n",
    "        if p_token in positions:\n",
    "            positions[p_token].append(i)\n",
    "\n",
    "    if any(not pos for pos in positions.values()):\n",
    "        return 1e6\n",
    "\n",
    "    all_found_positions = [pos for sublist in positions.values() for pos in sublist]\n",
    "    if not all_found_positions:\n",
    "        return 1e6\n",
    "        \n",
    "    proximity = max(all_found_positions) - min(all_found_positions)\n",
    "    return proximity / len(passage_tokens) if passage_tokens else 1e6 - len([token for token in positions.keys() if len(positions[token]) > 0])\n",
    "\n",
    "def generate_feature_vector(query_tokens, passage_tokens, query_embedding, passage_embedding):\n",
    "    cosine_sim = util.pytorch_cos_sim(query_embedding, passage_embedding).item()\n",
    "    \n",
    "    passage_tokens_list = passage_tokens.split()\n",
    "    word_in_beginning = int(any(token in passage_tokens_list[:10] for token in query_tokens.split()))\n",
    "    \n",
    "    query_token_set = set(query_tokens.split())\n",
    "    passage_token_set = set(passage_tokens_list)\n",
    "    query_coverage = len(query_token_set & passage_token_set) / len(query_token_set) if query_token_set else 0\n",
    "    \n",
    "    query_density = calculate_query_density(query_tokens.split(), passage_tokens_list)\n",
    "    \n",
    "    term_proximity = calculate_term_proximity(query_tokens.split(), passage_tokens_list)\n",
    "\n",
    "    token_in_passage = sum(passage_tokens_list.count(token) for token in query_tokens.split())\n",
    "    \n",
    "    return [cosine_sim, token_in_passage, word_in_beginning, query_coverage, query_density, term_proximity]\n",
    "\n",
    "def create_ranking_dataset(dataset, stemmed_queries, query_emb_map, stemmed_passages, passage_emb_map):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for ex in dataset:\n",
    "        query_id = ex['query_id']\n",
    "        if query_id not in stemmed_queries or not ('passages' in ex and ex['passages']['passage_text']):\n",
    "            continue\n",
    "\n",
    "        candidates_for_query = []\n",
    "        labels_for_query = []\n",
    "        \n",
    "        for i, ptext in enumerate(ex['passages']['passage_text']):\n",
    "            p_hash = get_text_hash(ptext)\n",
    "            if p_hash not in stemmed_passages:\n",
    "                continue\n",
    "            \n",
    "            feature_vector = generate_feature_vector(\n",
    "                stemmed_queries[query_id],\n",
    "                stemmed_passages[p_hash],\n",
    "                query_emb_map[query_id],\n",
    "                passage_emb_map[p_hash]\n",
    "            )\n",
    "            candidates_for_query.append(feature_vector)\n",
    "            labels_for_query.append(ex['passages']['is_selected'][i])\n",
    "\n",
    "        if candidates_for_query:\n",
    "            X.extend(candidates_for_query)\n",
    "            y.extend(labels_for_query)\n",
    "            \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание *обучающей* и *валидационной* выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T11:55:09.816344Z",
     "iopub.status.busy": "2025-09-16T11:55:09.815854Z",
     "iopub.status.idle": "2025-09-16T11:58:38.524263Z",
     "shell.execute_reply": "2025-09-16T11:58:38.523683Z",
     "shell.execute_reply.started": "2025-09-16T11:55:09.816321Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: X=(676193, 6), y=(676193,)\n",
      "Размер валидационной выборки: X=(82360, 6), y=(82360,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = create_ranking_dataset(train, stemmed_query, query_emb_map, stemmed_passages, passage_emb_map)\n",
    "print(f\"Размер обучающей выборки: X={X_train.shape}, y={y_train.shape}\")\n",
    "\n",
    "X_valid, y_valid = create_ranking_dataset(valid, stemmed_query_valid, query_emb_map_valid, stemmed_passages_valid, passage_emb_map_valid)\n",
    "print(f\"Размер валидационной выборки: X={X_valid.shape}, y={y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Модель обучена.\")\n",
    "\n",
    "feature_names = [\"cosine_sim\", \"word_in_beginning\", \"query_coverage\", \"query_density\", \"term_proximity\"]\n",
    "print(\"\\nВеса модели (коэффициенты):\")\n",
    "for name, coef in zip(feature_names, model.coef_):\n",
    "    print(f\"  - {name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка mrr модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:05:20.046400Z",
     "iopub.status.busy": "2025-09-16T13:05:20.046128Z",
     "iopub.status.idle": "2025-09-16T13:09:16.023761Z",
     "shell.execute_reply": "2025-09-16T13:09:16.023126Z",
     "shell.execute_reply.started": "2025-09-16T13:05:20.046380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Расчет метрик качества ---\n",
      "\n",
      "--- РЕЗУЛЬТАТЫ ---\n",
      "MRR (LinearRegression) на Обучающей выборке (Train): 0.5344\n",
      "MRR (LinearRegression) на Валидационной выборке (Valid): 0.5358\n"
     ]
    }
   ],
   "source": [
    "def evaluate_mrr(dataset, model, stemmed_queries, query_emb_map, stemmed_passages, passage_emb_map):\n",
    "    reciprocal_ranks = []\n",
    "    for ex in dataset:\n",
    "        query_id = ex['query_id']\n",
    "        if query_id not in stemmed_queries or not ('passages' in ex and ex['passages']['passage_text']):\n",
    "            continue\n",
    "\n",
    "        candidate_features = []\n",
    "        true_relevant_idx = -1\n",
    "        \n",
    "        for i, ptext in enumerate(ex['passages']['passage_text']):\n",
    "            p_hash = get_text_hash(ptext)\n",
    "            if p_hash not in stemmed_passages:\n",
    "                continue\n",
    "                \n",
    "            feature_vector = generate_feature_vector(\n",
    "                stemmed_queries[query_id],\n",
    "                stemmed_passages[p_hash],\n",
    "                query_emb_map[query_id],\n",
    "                passage_emb_map[p_hash]\n",
    "            )\n",
    "            candidate_features.append(feature_vector)\n",
    "            \n",
    "            if ex['passages']['is_selected'][i] == 1:\n",
    "                true_relevant_idx = len(candidate_features) - 1\n",
    "\n",
    "        if not candidate_features or true_relevant_idx == -1:\n",
    "            reciprocal_ranks.append(0)\n",
    "            continue\n",
    "\n",
    "        scores = model.predict(np.array(candidate_features))\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        \n",
    "        try:\n",
    "            rank = np.where(sorted_indices == true_relevant_idx)[0][0] + 1\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        except IndexError:\n",
    "            reciprocal_ranks.append(0)\n",
    "\n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0\n",
    "\n",
    "print(\"\\n--- Расчет метрик качества ---\")\n",
    "\n",
    "# Оценка на обучающей выборке\n",
    "mrr_train = evaluate_mrr(train, model, stemmed_query, query_emb_map, stemmed_passages, passage_emb_map)\n",
    "\n",
    "# Оценка на валидационной выборке\n",
    "mrr_valid = evaluate_mrr(valid, model, stemmed_query_valid, query_emb_map_valid, stemmed_passages_valid, passage_emb_map_valid)\n",
    "\n",
    "print(\"\\n--- РЕЗУЛЬТАТЫ ---\")\n",
    "print(f\"MRR (LinearRegression) на Обучающей выборке (Train): {mrr_train:.4f}\")\n",
    "print(f\"MRR (LinearRegression) на Валидационной выборке (Valid): {mrr_valid:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
